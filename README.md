# Saboteur LLM Agent Benchmark

This is the implementation of the paper: `SABOTEUR: EVALUATING STRATEGIC DECISION-MAKING IN MULTI-AGENT ADVERSARIAL SETTINGS`

**Authors:** Jan Cuhel, Jennifer Za Nzambi 

**Abstract:** Inspired by the card game Saboteur, this paper presents a benchmark evaluating
strategic decision-making in multi-agent settings where large language models
(LLMs) take on the roles of players. Unlike many other benchmarks focusing
on static scenarios, our evaluation introduces a scalable and dynamic framework
offering nuanced insights into how LLMs develop sophisticated deception me-
chanics, engage in long-term strategizing, and exhibit collaboration skills. On
an illustrative example of Claude 3.5 Haiku and GPT-4o Mini it is demonstrated
that whilst the former is more apt at detecting adverse agents, GPT-4o Mini has a
greater win rate, thereby shedding light on how LLMs balance deception detection
with strategic planning and adversarial dynamics in multi-agent settings.

The code will be released soon.
